<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Kwanyoung Park</title>

    <meta name="author" content="Kwanyoung Park">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/profile.jpg" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Kwanyoung Park
                </p>
                <p>
                   I'm an undergraduate in <a href="https://www.snu.ac.kr/index.html">Seoul National University</a>, actively engaged in research within the field of Reinforcement Learning (RL). 
                   Currently, I am tackling offline RL problems at Yonsei University's <a href="https://youngwoon.github.io/students">RLLab</a> under the guidance of <a href="https://youngwoon.github.io/">Prof. Youngwoon Lee</a>. 
                </p>
                <p>
                   Previously, I have done a research internship at <a href="https://hcs.snu.ac.kr">HCS Lab</a> in Seoul National Univesity with <a href="https://youngkilee.blogspot.com/">Prof. Youngki Lee</a>. 
                   During that time, my focus was on bridging the gap between machine learning processes and human-like learning processes. 
                   Additionally, I contributed to the development of NeRF models tailored for on-device applications.
                </p>
                <p style="text-align:center">
                  <a href="mailto:williampark202@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/KwanyoungPark-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/kwanyoung-park-96b1b1182/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/GGOSinon/">Github</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=odFC9mAAAAAJ">Google Scholar</a> 
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/profile.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                The goal of my research is to bridge the gap between machine learning processes and human-like learning processes, focusing on improving data efficiency of reinforcement learning agents.
                </p>
                <p>
                So far, I have explored the differences and similarities between reinforcement learning agents and cognition of human agents (toddlers).
                Currently, I'm focusing on using offline data to accelerate the learning process of the agent.

                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    </video></div>
      <img src='images/VECA.jpg' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <span class="papertitle">VECA: A New Benchmark and Toolkit for General Cognitive Development</span>
    <br>
        <font color="purple"><strong>Kwanyoung Park*</strong></font>,
	<a href="http://www.aistudy.com/ohs/index.html">Hyunseok Oh*</a>,
	<a href="https://youngkilee.blogspot.com/">Youngki Lee</a>
    <br>
    <em><strong>AAAI, 2022</strong><em> &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
    <br>

    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/19877">paper</a> / 
    <a href="https://github.com/GGOSinon/VECA">code</a>
    <br>
    <p>
      VECA is a toolkit for general cognitive development of artificial cognitive agents.
      It provides toolkit to build new tasks to train cognitive agents, and also a standardized benchmark to test the performance of the trained agents based on the Bayley-4 assessment.
    </p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    </video></div>
      <img src='images/CriticalPeriod.jpg' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <span class="papertitle">Toddler-Guidance Learning: Impacts of Critical Period on Multimodal AI Agents</span>
    <br>
	<a href="https://scholar.google.co.kr/citations?user=UiPPWGkAAAAJ">Junseok Park</a>,
	<font color="purple"><strong>Kwanyoung Park</strong></font>,
	<a href="http://www.aistudy.com/ohs/index.html">Hyunseok Oh</a>,
	<a href="https://scholar.google.com/citations?user=wN-2e1MAAAAJ">Ganghun Lee</a>,
	<a href="https://scholar.google.com/citations?user=75_DkUwAAAAJ">Minsu Lee</a>,
	<a href="https://youngkilee.blogspot.com/">Youngki Lee</a>
	<a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang</a>
    <br>
    <em><strong>ICMI, 2021</strong><em> &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
    <br>
    <a href="https://arxiv.org/abs/2201.04990">paper</a> / 
    <a href="">code</a>
    <br>
    <p>
      We investigate the emergence of critical periods on multimodal reinforcement learning.
      We show the performance on RL task and transfer learning depends on what and when the guidance is given to the agent.
    </p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    </video></div>
      <img src='images/ToddlerLearning.jpg' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <span class="papertitle">Learning task-agnostic representation via toddler-inspired learning</span>
    <br>
    <font color="purple"><strong>Kwanyoung Park</strong></font>,
	<a href="https://scholar.google.co.kr/citations?user=UiPPWGkAAAAJ">Junseok Park</a>,
	<a href="http://www.aistudy.com/ohs/index.html">Hyunseok Oh</a>,
	<a href="https://youngkilee.blogspot.com/">Youngki Lee</a>,
	<a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang</a>
    <br>
    <em><strong>NeurIPS Workshop, 2020</strong><em>
    <br>
    <a href="https://arxiv.org/abs/2101.11221">paper</a> / 
    <a href="">code</a>
    <br>
    <p>
      Toddler's learning procedure consists of interactive experiences, resulting in task-agnostic representations.
      Inspired by those precedures, we pretrain the agent on a visual navigation task and show that the representations obtained during the RL task is expandable to various vision tasks.
    </p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:100%;vertical-align:middle" colspan="2">
    <p style="text-align:right">
      Template from <a href="https://github.com/jonbarron/website">Jon Barron</a>
    </p>
  </td>
</tr>

  </body>
</html>
