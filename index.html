<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Kwanyoung Park</title>

    <meta name="author" content="Kwanyoung Park">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/profile.jpg" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Kwanyoung Park
                </p>
                <p>
                   Hello! I'm Kwanyoung, incoming PhD at UC Berkeley advised by Prof. <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a> and Prof. <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>.
                   Currently, I am tackling offline RL problems with model-based approaches at Yonsei University's <a href="https://youngwoon.github.io/">RLLAB</a> under the guidance of Prof. <a href="https://youngwoon.github.io/">Youngwoon Lee</a>. 
                </p>
                <p>
                   <!--
                   I'm currently an undergraduate in <a href="https://www.snu.ac.kr/index.html">Seoul National University</a>, majoring in Computer Science and Engineering, with a minor in Mathematical Sciences. -->
                   Previously, I have done a research internship at <a href="https://hcs.snu.ac.kr">Human-centered Computer Systems Lab</a> in Seoul National University with Prof. <a href="https://youngkilee.blogspot.com/">Youngki Lee</a>. 
                   During the internship, my primary focus was on bridging the gap between machine learning processes and human-like learning processes. 
                </p>
                <p style="text-align:center">
                  <a href="mailto:williampark202@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/KwanyoungPark-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=odFC9mAAAAAJ">Google Scholar</a> &nbsp;/&nbsp; 
                  <a href="https://github.com/kwanyoungpark/">Github</a> &nbsp;/&nbsp; 
                  <a href="https://x.com/kwanyoung_park_">Twitter</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/profile.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                    The goal of my research is to develop a <strong>"foundational embodied agent"</strong>, an embodied agent that is generalizable and rapidly adapt to novel tasks with few demonstrations.
                    To enable this, foundational models must develop priors about the world, capturing task distributions (e.g., manipulation, goal-reaching) and environmental dynamics (e.g., physical properties).
                    
                    <p></p>
                    
                    My current focus is on three key approaches:

                    <ul class="a">
                    <li> <strong>World Models</strong>: 
                    World models, which can be trained on abundant task-agnostic data, offer richer and grounded learning signals via simulated trajectories, enabling improved generalization and planning. 
                    However, world models are often inaccurate, especially in capturing physical knowledge critical for embodied agents.
                    How can we address these inaccuracies in model-based RL, and how can we train better world models?

                    <p></p>

                    <li> <strong>Offline RL</strong>:
                    While behavior cloning (BC) with large demonstration dataset have proven instrumental in building foundation models in robotics, it cannot learn stitching or corrective behaviors, requiring extensive optimal data.
                    Can we leverage offline RL to utilize suboptimal datasets for training and fine-tuning foundational embodied agents?

                    <p></p>

                    <li> <strong>Unsupervised RL</strong>:
                    Unsupervised RL enables task-agnostic pretraining through intrinsic rewards. 
                    Similar to the success of unsupervised pretraining in vision and NLP domains, can we train transferable representations and skills in RL and effectively apply them to downstream tasks?
                    </ul>
                </p>
              </td>
            </tr>
              <td style="padding:20px;width:100%;vertical-align:middle;line-height:10px">
                <h2 style="padding-bottom:0px">Publications</h2>
                <p style="padding-top:0px">(* denotes equal contribution)</p>
              </td>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:-20px"><tbody>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    </video></div>
      <img src='images/LEQ_thumbnail.gif' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <span class="papertitle">Model-based Offline Reinforcement Learning with Lower Expectile Q-Learning</span>
    <br>
        <font color="purple"><strong>Kwanyoung Park</strong></font>,
        <a href="https://youngwoon.github.io/">Youngwoon Lee</a>
    <br>
    <em><strong>ICLR, 2025</strong><em> 
    <br>
        <a href="https://arxiv.org/abs/2407.00699">paper</a> / 
        <a href="https://github.com/kwanyoungpark/LEQ">code</a> /
        <a href="https://kwanyoungpark.github.io/LEQ">website</a> / 
        <a href="https://x.com/kwanyoung_park_/status/1810308456131547289">twitter</a>
    <br>
    <p>
        We introduce a novel model-based offline RL method, Lower Expectile Q-learning (LEQ), which enhances long-horizon task performance by mitigating the high bias in model-based value estimation via expectile regression of Î»-returns.
        Our empirical results show that LEQ significantly outperforms previous model-based offline RL methods on long-horizon tasks, such as the D4RL AntMaze tasks, matching or surpassing the performance of model-free approaches.
    </p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    </video></div>
      <img src='images/TLDR_thumbnail.gif' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <span class="papertitle">TLDR: Unsupervised Goal-Conditioned RL via Temporal Distance-Aware Representations</span>
    <br>
        <a href="https://heatz123.github.io/">Junik Bae</a>, 
        <font color="purple"><strong>Kwanyoung Park</strong></font>,
        <a href="https://youngwoon.github.io/">Youngwoon Lee</a>
    <br>
    <em><strong>CoRL, 2024</strong><em> 
    <br>
        <a href="https://arxiv.org/abs/2407.08464">paper</a> / 
        <a href="https://github.com/heatz123/tldr">code</a> /
        <a href="https://heatz123.github.io/tldr">website</a> / 
        <a href="https://x.com/junikbae/status/1812800490436231188">twitter</a>
    <br>
    <p>
        We propose a novel unsupervised goal-conditioned RL method, TLDR, which leverages TemporaL Distance-aware Representations.
        Our approach selects faraway goals to initiate exploration and compute intrinsic exploration rewards and goal-reaching rewards.
        Our experimental results in robotic locomotion and manipulation environments demonstrate that our method significantly outperforms previous unsupervised GCRL methods in achieving a wide variety of states.
    </p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    </video></div>
      <img src='images/VECA.jpg' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <span class="papertitle">VECA: A New Benchmark and Toolkit for General Cognitive Development</span>
    <br>
        <font color="purple"><strong>Kwanyoung Park*</strong></font>,
	<a href="http://www.aistudy.com/ohs/index.html">Hyunseok Oh*</a>,
	<a href="https://youngkilee.blogspot.com/">Youngki Lee</a>
    <br>
    <em><strong>AAAI, 2022</strong><em> &nbsp <font color="red"><strong>(Oral Presentation, Acceptance Rate: 384/9,251 = 4.15%)</strong></font>
    <br>

    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/19877">paper</a> / 
    <a href="https://github.com/kwanyoungpark/VECA">code</a>
    <br>
    <p>
      We present VECA(Virtual Environment for Cognitive Assessment), which consists of two main components: (i) a first benchmark to assess the overall cognitive development of an AI agent, and (ii) a novel toolkit to generate diverse and distinct cognitive tasks. 
      VECA benchmark virtually implements the cognitive scale of Bayley Scales of Infant and Toddler Development-IV(Bayley-4), the gold-standard developmental assessment for human infants and toddlers.
    </p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    </video></div>
      <img src='images/CriticalPeriod.jpg' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <span class="papertitle">Toddler-Guidance Learning: Impacts of Critical Period on Multimodal AI Agents</span>
    <br>
	<a href="https://scholar.google.co.kr/citations?user=UiPPWGkAAAAJ">Junseok Park</a>,
	<font color="purple"><strong>Kwanyoung Park</strong></font>,
	<a href="http://www.aistudy.com/ohs/index.html">Hyunseok Oh</a>,
	<a href="https://scholar.google.com/citations?user=wN-2e1MAAAAJ">Ganghun Lee</a>,
	<a href="https://scholar.google.com/citations?user=75_DkUwAAAAJ">Minsu Lee</a>,
	<a href="https://youngkilee.blogspot.com/">Youngki Lee</a>,
	<a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang</a>
    <br>
    <em><strong>ICMI, 2021</strong><em> &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
    <br>
    <a href="https://arxiv.org/abs/2201.04990">paper</a> / 
    <a href="">code</a>
    <br>
    <p>
      We investigate the emergence of critical periods on multimodal reinforcement learning.
      We show the performance on RL task and transfer learning depends on what and when the guidance is given to the agent.
    </p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    </video></div>
      <img src='images/ToddlerLearning.jpg' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <span class="papertitle">Learning task-agnostic representation via toddler-inspired learning</span>
    <br>
    <font color="purple"><strong>Kwanyoung Park</strong></font>,
	<a href="https://scholar.google.co.kr/citations?user=UiPPWGkAAAAJ">Junseok Park</a>,
	<a href="http://www.aistudy.com/ohs/index.html">Hyunseok Oh</a>,
	<a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang</a>,
	<a href="https://youngkilee.blogspot.com/">Youngki Lee</a>
    <br>
    <em><strong>NeurIPS Workshop, 2020</strong><em>
    <br>
    <a href="https://arxiv.org/abs/2101.11221">paper</a> / 
    <a href="">code</a>
    <br>
    <p>
      Toddler's learning procedure consists of interactive experiences, resulting in task-agnostic representations.
      Inspired by those precedures, we pretrain the agent on a visual navigation task and show that the representations obtained during the RL task is expandable to various vision tasks.
    </p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:100%;vertical-align:middle" colspan="2">
    <p style="text-align:right">
      Website template from <a href="https://github.com/jonbarron/website">Jon Barron</a>.
    </p>
  </td>
</tr>

  </body>
</html>
