<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Kwanyoung Park</title>

    <meta name="author" content="Kwanyoung Park">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/profile.jpg" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Kwanyoung Park
                </p>
                <p>
                   Hello! I'm Kwanyoung, a first-year PhD student at UC Berkeley, advised by Prof. <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a> and Prof. <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>.
                </p>
                <p>
                   Before joining Berkeley, I was a research intern at <a href="https://youngwoon.github.io/">RLLAB</a> in Yonsei University, working with Prof. <a href="https://youngwoon.github.io/">Youngwoon Lee</a> on tackling offline RL problems with model-based approaches, and unsupervised RL problems. 
                   Previously, I have done a research internship at <a href="https://hcs.snu.ac.kr">Human-centered Computer Systems Lab</a> in Seoul National University with Prof. <a href="https://youngkilee.blogspot.com/">Youngki Lee</a>, where I explored connections between machine learning processes and human learning.
                </p>
                <p style="text-align:center">
                  <a href="mailto:kwanyoung_park@berkeley.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/KwanyoungPark-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=odFC9mAAAAAJ">Google Scholar</a> &nbsp;/&nbsp; 
                  <a href="https://github.com/kwanyoungpark/">Github</a> &nbsp;/&nbsp; 
                  <a href="https://x.com/kwanyoung_park_">X (Twitter)</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/profile.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                    The goal of my research is to develop a <strong>"foundational embodied agent"</strong>, an embodied agent that is generalizable and rapidly adapt to novel tasks with few demonstrations.
                    To enable this, foundational models must develop priors about the world, capturing task distributions (e.g., manipulation, goal-reaching) and environmental dynamics (e.g., physical properties).
                    
                    <p></p>
                    
                    My current focus is on three key approaches:

                    <ul class="a">
                    <li> <strong>World Models</strong>: 
                    World models, which can be trained on abundant task-agnostic data, offer richer and grounded learning signals via simulated trajectories, enabling improved generalization and planning. 
                    However, world models are often inaccurate, especially in capturing physical knowledge critical for embodied agents.
                    How can we address these inaccuracies in model-based RL, and how can we train better world models?

                    <p></p>

                    <li> <strong>Offline RL</strong>:
                    While behavior cloning (BC) with large demonstration dataset have proven instrumental in building foundation models in robotics, it cannot learn stitching or corrective behaviors, requiring extensive optimal data.
                    Can we leverage offline RL to utilize suboptimal datasets for training and fine-tuning foundational embodied agents?

                    <p></p>

                    <li> <strong>Unsupervised RL</strong>:
                    Unsupervised RL enables task-agnostic pretraining through intrinsic rewards. 
                    Similar to the success of unsupervised pretraining in vision and NLP domains, can we train transferable representations and skills in RL and effectively apply them to downstream tasks?
                    </ul>
                </p>
              </td>
            </tr>
              <td style="padding:20px;width:100%;vertical-align:middle;line-height:10px">
                <h2 style="padding-bottom:0px">Publications</h2>
                <p style="padding-top:0px">(* denotes equal contribution)</p>
              </td>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:-20px"><tbody>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    </video></div>
      <img src='images/MAC_thumbnail.gif' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <span class="papertitle">Scalable Offline Model-Based RL with Action Chunking</span>
    <br>
        <font color="purple"><strong>Kwanyoung Park</strong></font>,
        <a href="https://seohong.me/">Seohong Park</a>,
        <a href="https://youngwoon.github.io/">Youngwoon Lee</a>,
        <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
    <br>
    <em><strong>Arxiv, 2025</strong><em> 
    <br>
        <a href="https://arxiv.org/abs/2512.08108">paper</a> /
        <a href="https://github.com/kwanyoungpark/MAC">code</a> /
        <a href="https://kwanyoungpark.github.io/MAC">website</a> / 
        thread 
    <br>
    <p>
         We propose Model-Based RL with Action Chunks (MAC), a simple yet powerful offline model-based RL method that leverages 100-step model rollouts for value estimation, by utilizing rejection sampling from an expressive, flow-based, action-chunked behavior cloning (BC) policy. 
         MAC achieves strong performance and scales effectively to large datasets, significantly outperforming prior model-based approaches on both standard and 100M-scale OGBench datasets.
    </p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    </video></div>
      <img src='images/LEQ_thumbnail.gif' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <span class="papertitle">Model-based Offline Reinforcement Learning with Lower Expectile Q-Learning</span>
    <br>
        <font color="purple"><strong>Kwanyoung Park</strong></font>,
        <a href="https://youngwoon.github.io/">Youngwoon Lee</a>
    <br>
    <em><strong>ICLR, 2025</strong><em> 
    <br>
        <a href="https://arxiv.org/abs/2407.00699">paper</a> / 
        <a href="https://github.com/kwanyoungpark/LEQ">code</a> /
        <a href="https://kwanyoungpark.github.io/LEQ">website</a> / 
        <a href="https://x.com/kwanyoung_park_/status/1810308456131547289">thread</a>
    <br>
    <p>
        We introduce a novel model-based offline RL method, Lower Expectile Q-learning (LEQ), which enhances long-horizon task performance by mitigating the high bias in model-based value estimation via expectile regression of Î»-returns.
        Our empirical results show that LEQ significantly outperforms previous model-based offline RL methods on long-horizon tasks, such as the D4RL AntMaze tasks, matching or surpassing the performance of model-free approaches.
    </p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    </video></div>
      <img src='images/TLDR_thumbnail.gif' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <span class="papertitle">TLDR: Unsupervised Goal-Conditioned RL via Temporal Distance-Aware Representations</span>
    <br>
        <a href="https://heatz123.github.io/">Junik Bae</a>, 
        <font color="purple"><strong>Kwanyoung Park</strong></font>,
        <a href="https://youngwoon.github.io/">Youngwoon Lee</a>
    <br>
    <em><strong>CoRL, 2024</strong><em> 
    <br>
        <a href="https://arxiv.org/abs/2407.08464">paper</a> / 
        <a href="https://github.com/heatz123/tldr">code</a> /
        <a href="https://heatz123.github.io/tldr">website</a> / 
        <a href="https://x.com/junikbae/status/1812800490436231188">thread</a>
    <br>
    <p>
        We propose a novel unsupervised goal-conditioned RL method, TLDR, which leverages TemporaL Distance-aware Representations.
        Our approach selects faraway goals to initiate exploration and compute intrinsic exploration rewards and goal-reaching rewards.
        Our experimental results in robotic locomotion and manipulation environments demonstrate that our method significantly outperforms previous unsupervised GCRL methods in achieving a wide variety of states.
    </p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    </video></div>
      <img src='images/VECA.jpg' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <span class="papertitle">VECA: A New Benchmark and Toolkit for General Cognitive Development</span>
    <br>
        <font color="purple"><strong>Kwanyoung Park*</strong></font>,
	<a href="http://www.aistudy.com/ohs/index.html">Hyunseok Oh*</a>,
	<a href="https://youngkilee.blogspot.com/">Youngki Lee</a>
    <br>
    <em><strong>AAAI, 2022</strong><em> &nbsp <font color="red"><strong>(Oral Presentation, Acceptance Rate: 384/9,251 = 4.15%)</strong></font>
    <br>

    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/19877">paper</a> / 
    <a href="https://github.com/kwanyoungpark/VECA">code</a>
    <br>
    <p>
      We present VECA(Virtual Environment for Cognitive Assessment), which consists of two main components: (i) a first benchmark to assess the overall cognitive development of an AI agent, and (ii) a novel toolkit to generate diverse and distinct cognitive tasks. 
      VECA benchmark virtually implements the cognitive scale of Bayley Scales of Infant and Toddler Development-IV(Bayley-4), the gold-standard developmental assessment for human infants and toddlers.
    </p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    </video></div>
      <img src='images/CriticalPeriod.jpg' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <span class="papertitle">Toddler-Guidance Learning: Impacts of Critical Period on Multimodal AI Agents</span>
    <br>
	<a href="https://scholar.google.co.kr/citations?user=UiPPWGkAAAAJ">Junseok Park</a>,
	<font color="purple"><strong>Kwanyoung Park</strong></font>,
	<a href="http://www.aistudy.com/ohs/index.html">Hyunseok Oh</a>,
	<a href="https://scholar.google.com/citations?user=wN-2e1MAAAAJ">Ganghun Lee</a>,
	<a href="https://scholar.google.com/citations?user=75_DkUwAAAAJ">Minsu Lee</a>,
	<a href="https://youngkilee.blogspot.com/">Youngki Lee</a>,
	<a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang</a>
    <br>
    <em><strong>ICMI, 2021</strong><em> &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
    <br>
    <a href="https://arxiv.org/abs/2201.04990">paper</a> / 
    <a href="">code</a>
    <br>
    <p>
      We investigate the emergence of critical periods on multimodal reinforcement learning.
      We show the performance on RL task and transfer learning depends on what and when the guidance is given to the agent.
    </p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    </video></div>
      <img src='images/ToddlerLearning.jpg' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <span class="papertitle">Learning task-agnostic representation via toddler-inspired learning</span>
    <br>
    <font color="purple"><strong>Kwanyoung Park</strong></font>,
	<a href="https://scholar.google.co.kr/citations?user=UiPPWGkAAAAJ">Junseok Park</a>,
	<a href="http://www.aistudy.com/ohs/index.html">Hyunseok Oh</a>,
	<a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang</a>,
	<a href="https://youngkilee.blogspot.com/">Youngki Lee</a>
    <br>
    <em><strong>NeurIPS Workshop, 2020</strong><em>
    <br>
    <a href="https://arxiv.org/abs/2101.11221">paper</a> / 
    <a href="">code</a>
    <br>
    <p>
      Toddler's learning procedure consists of interactive experiences, resulting in task-agnostic representations.
      Inspired by those precedures, we pretrain the agent on a visual navigation task and show that the representations obtained during the RL task is expandable to various vision tasks.
    </p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:100%;vertical-align:middle" colspan="2">
    <p style="text-align:right">
      Website template from <a href="https://github.com/jonbarron/website">Jon Barron</a>.
    </p>
  </td>
</tr>

  </body>
</html>
