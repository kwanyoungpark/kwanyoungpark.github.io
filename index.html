<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Kwanyoung Park</title>

    <meta name="author" content="Kwanyoung Park">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/profile.jpg" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Kwanyoung Park
                </p>
                <p>
                   I'm an undergraduate in <a href="https://www.snu.ac.kr/index.html">Seoul National University</a>, actively engaging in research within the field of Reinforcement Learning (RL). 
                   I'm majoring in Computer Science and Engineering, and doing a minor of Mathematical Sciences.
                   Currently, I am tackling offline RL problems at Yonsei University's <a href="https://youngwoon.github.io/">RLLAB</a> under the guidance of <a href="https://youngwoon.github.io/">Youngwoon Lee</a>. 
                </p>
                <p>
                   Previously, I have done a research internship at <a href="https://hcs.snu.ac.kr">Human-centered Computer Systems Lab</a> in Seoul National University with <a href="https://youngkilee.blogspot.com/">Youngki Lee</a>. 
                   During the internship, my primary focus was on bridging the gap between machine learning processes and human-like learning processes. 
                   Additionally, I contributed to the development of NeRF models tailored for on-device applications.
                </p>
                <p style="text-align:center">
                  <a href="mailto:williampark202@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/KwanyoungPark-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=odFC9mAAAAAJ">Google Scholar</a> &nbsp;/&nbsp; 
                  <a href="https://www.linkedin.com/in/kwanyoung-park-96b1b1182/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/GGOSinon/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/profile.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                    The goal of my research is to bridge the gap between machine learning processes and human-like learning processes, focusing on improving data efficiency of reinforcement learning agents.
                </p>
                <p>
                    So far, I have explored the differences and similarities between reinforcement learning agents and cognition of human agents (toddlers).
                    Currently, I'm focusing on using offline data to accelerate the learning process of the agent.
                </p>
              </td>
            </tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publication & Preprints</h2>
              </td>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:-20px"><tbody>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    </video></div>
      <img src='images/LEQ_thumbnail.gif' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <span class="papertitle">Tackling Long-Horizon Tasks with Model-based Offline Reinforcement Learning</span>
    <br>
        <font color="purple"><strong>Kwanyoung Park</strong></font>,
        <a href="https://youngwoon.github.io/">Youngwoon Lee</a>
    <br>
    <em><strong>Preprint, 2024</strong><em> 
    <br>
        <a href="https://arxiv.org/abs/2407.00699">paper</a> / 
        <a href="https://github.com/kwanyoungpark/LEQ">code</a> /
        <a href="https://kwanyoungpark.github.io/LEQ">project page</a>  
    <br>
    <p>
        We introduce a novel model-based offline RL method, Lower Expectile Q-learning (LEQ), which enhances long-horizon task performance by mitigating the high bias in model-based value estimation via expectile regression of Î»-returns.
        Our empirical results show that LEQ significantly outperforms previous model-based offline RL methods on long-horizon tasks, such as the D4RL AntMaze tasks, matching or surpassing the performance of model-free approaches.
    </p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    </video></div>
      <img src='images/TLDR_thumbnail.gif' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <span class="papertitle">TLDR: Unsupervised Goal-Conditioned RL via Temporal Distance-Aware Representations</span>
    <br>
        <a href="https://heatz123.github.io/">Junik Bae</a>, 
        <font color="purple"><strong>Kwanyoung Park</strong></font>,
        <a href="https://youngwoon.github.io/">Youngwoon Lee</a>
    <br>
    <em><strong>Preprint, 2024</strong><em> 
    <br>
        <a href="">paper</a> / 
        <a href="">code</a>
    <br>
    <p>
        We propose a novel unsupervised goal-conditioned RL method, TLDR, which leverages TemporaL Distance-aware Representations.
        Our approach selects faraway goals to initiate exploration and compute intrinsic exploration rewards and goal-reaching rewards.
        Our experimental results in robotic locomotion and manipulation environments demonstrate that our method significantly outperforms previous unsupervised GCRL methods in achieving a wide variety of states.
    </p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    </video></div>
      <img src='images/VECA.jpg' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <span class="papertitle">VECA: A New Benchmark and Toolkit for General Cognitive Development</span>
    <br>
        <font color="purple"><strong>Kwanyoung Park*</strong></font>,
	<a href="http://www.aistudy.com/ohs/index.html">Hyunseok Oh*</a>,
	<a href="https://youngkilee.blogspot.com/">Youngki Lee</a>
    <br>
    <em><strong>AAAI, 2022</strong><em> &nbsp <font color="red"><strong>(Oral Presentation, Acceptance Rate: 384/9,251 = 4.15%)</strong></font>
    <br>

    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/19877">paper</a> / 
    <a href="https://github.com/GGOSinon/VECA">code</a>
    <br>
    <p>
      We present VECA(Virtual Environment for Cognitive Assessment), which consists of two main components: (i) a first benchmark to assess the overall cognitive development of an AI agent, and (ii) a novel toolkit to generate diverse and distinct cognitive tasks. 
      VECA benchmark virtually implements the cognitive scale of Bayley Scales of Infant and Toddler Development-IV(Bayley-4), the gold-standard developmental assessment for human infants and toddlers.
    </p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    </video></div>
      <img src='images/CriticalPeriod.jpg' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <span class="papertitle">Toddler-Guidance Learning: Impacts of Critical Period on Multimodal AI Agents</span>
    <br>
	<a href="https://scholar.google.co.kr/citations?user=UiPPWGkAAAAJ">Junseok Park</a>,
	<font color="purple"><strong>Kwanyoung Park</strong></font>,
	<a href="http://www.aistudy.com/ohs/index.html">Hyunseok Oh</a>,
	<a href="https://scholar.google.com/citations?user=wN-2e1MAAAAJ">Ganghun Lee</a>,
	<a href="https://scholar.google.com/citations?user=75_DkUwAAAAJ">Minsu Lee</a>,
	<a href="https://youngkilee.blogspot.com/">Youngki Lee</a>,
	<a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang</a>
    <br>
    <em><strong>ICMI, 2021</strong><em> &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
    <br>
    <a href="https://arxiv.org/abs/2201.04990">paper</a> / 
    <a href="">code</a>
    <br>
    <p>
      We investigate the emergence of critical periods on multimodal reinforcement learning.
      We show the performance on RL task and transfer learning depends on what and when the guidance is given to the agent.
    </p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    </video></div>
      <img src='images/ToddlerLearning.jpg' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <span class="papertitle">Learning task-agnostic representation via toddler-inspired learning</span>
    <br>
    <font color="purple"><strong>Kwanyoung Park</strong></font>,
	<a href="https://scholar.google.co.kr/citations?user=UiPPWGkAAAAJ">Junseok Park</a>,
	<a href="http://www.aistudy.com/ohs/index.html">Hyunseok Oh</a>,
	<a href="https://youngkilee.blogspot.com/">Youngki Lee</a>,
	<a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang</a>
    <br>
    <em><strong>NeurIPS Workshop, 2020</strong><em>
    <br>
    <a href="https://arxiv.org/abs/2101.11221">paper</a> / 
    <a href="">code</a>
    <br>
    <p>
      Toddler's learning procedure consists of interactive experiences, resulting in task-agnostic representations.
      Inspired by those precedures, we pretrain the agent on a visual navigation task and show that the representations obtained during the RL task is expandable to various vision tasks.
    </p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:100%;vertical-align:middle" colspan="2">
    <p style="text-align:right">
      Website template from <a href="https://github.com/jonbarron/website">Jon Barron</a>.
    </p>
  </td>
</tr>

  </body>
</html>
